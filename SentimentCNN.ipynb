{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow.keras\n",
    "from PIL import Image\n",
    "import h5py\n",
    "import glob\n",
    "import random\n",
    "import cv2\n",
    "import csv\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras \n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.preprocessing import image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting the directory of dataset\n",
    "IMG_DIR = \"C:\\\\Users\\\\JohnQ\\\\Documents\\\\Hackathons\\\\SwampHacks2020\\\\images\"\n",
    "DATA_DIR = \"C:\\\\Users\\\\JohnQ\\\\Documents\\\\Hackathons\\\\SwampHacks2020\\\\data\"\n",
    "NEWDATA_DIR = \"C:\\\\Users\\\\JohnQ\\\\Documents\\\\Hackathons\\\\SwampHacks2020\\\\resized_data\"\n",
    "img_row, img_cols = 200, 200\n",
    "\n",
    "#keep track of deleted images\n",
    "del_imgs = set()\n",
    "for file in os.listdir(IMG_DIR):\n",
    "    im = Image.open(IMG_DIR+'\\\\'+file)\n",
    "    if(im.size >= (200, 200)):  \n",
    "        #resize images\n",
    "        img = im.resize((img_row, img_cols))\n",
    "        #convert to grayscale\n",
    "        gray = im.convert('L')\n",
    "        gray.save(NEWDATA_DIR+'\\\\'+file,\"JPEG\")\n",
    "    else:\n",
    "        del_imgs.add(file)\n",
    "        continue\n",
    "\n",
    "#images = os.listdir(NEWDATA_DIR)\n",
    "#tester = array(Image.open('resized_data'+'\\\\'+image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = r\"./dsSwampers.hdf5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_DIR = \"C:\\\\Users\\\\JohnQ\\\\Documents\\\\Hackathons\\\\SwampHacks2020\\\\images\"\n",
    "DATA_DIR = \"C:\\\\Users\\\\JohnQ\\\\Documents\\\\Hackathons\\\\SwampHacks2020\\\\data\"\n",
    "NEWDATA_DIR = \"C:\\\\Users\\\\JohnQ\\\\Documents\\\\Hackathons\\\\SwampHacks2020\\\\resized_data\"\n",
    "def read_img(NEWDATA_DIR):\n",
    "    fileList = []\n",
    "\n",
    "    for rot, dirs, files in os.walk(NEWDATA_DIR):\n",
    "        for file in files:\n",
    "            if(file.endswith(\"jpg\") or file.endswith(\"JPG\")):\n",
    "                temp = str(NEWDATA_DIR+file)\n",
    "                fileList.append(temp)\n",
    "    return fileList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "thisDict = dict()\n",
    "files = os.listdir(DATA_DIR)\n",
    "with open(DATA_DIR+'\\\\'+'500_picts_satz.csv', mode = 'r') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file)\n",
    "    with open(DATA_DIR+'\\\\'+'new_data.csv', mode='w') as outfile:\n",
    "        writer = csv.writer(outfile)\n",
    "        thisDict = {IMG_DIR+'\\\\'+row[1]:row[2] for row in csv_reader if row[2] == 'happiness' or row[2] == 'neutral'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "happiness = []\n",
    "boredom = []\n",
    "for key in thisDict:\n",
    "    if(thisDict[key] == 'happiness'):\n",
    "        happiness.append(key)\n",
    "    else:\n",
    "        boredom.append(key)\n",
    "hapAlt = random.sample(happiness, len(boredom))\n",
    "boreAlt = random.sample(boredom, len(boredom))\n",
    "\n",
    "label_bumps = np.ones(shape=(len(hapAlt),), dtype=np.uint8).tolist()\n",
    "label_trash = np.zeros(shape=(len(boreAlt),), dtype = np.uint8).tolist()\n",
    "\n",
    "img_addr = hapAlt\n",
    "img_label = label_bumps + label_trash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches = list(zip(img_addr, img_label))\n",
    "random.shuffle(matches)\n",
    "(addrs, labels) = zip(*matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84 84\n"
     ]
    }
   ],
   "source": [
    "train_addr = list(addrs[0:int(len(addrs))])\n",
    "train_labels = list(labels[0:int(len(labels))])\n",
    "print(len(train_addr), len(train_labels))\n",
    "img_shape = (len(img_addr),100, 100, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<HDF5 dataset \"train_img\": shape (84, 100, 100, 3), type \"<f4\">\n",
      "<HDF5 dataset \"train_label\": shape (84,), type \"<f4\">\n"
     ]
    }
   ],
   "source": [
    "f = h5py.File(test, mode=\"w\")\n",
    "f.create_dataset(\"train_img\", img_shape, np.float32)\n",
    "f.create_dataset(\"train_label\", (len(train_labels), ), np.float32)\n",
    "f[\"train_label\"][...] = train_labels\n",
    "x,y = f.values()\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "bad_indexes = []\n",
    "for i in range(len(img_addr)):\n",
    "  \n",
    "    addr_read = img_addr[i]\n",
    "    img = cv2.imread(addr_read)\n",
    "    img = cv2.resize(img, (100,100), interpolation=cv2.INTER_CUBIC)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    f[\"train_img\"][i, ...] = img[None]\n",
    "\n",
    "    #resize images\n",
    "    #img = img.resize((img_row, img_cols))\n",
    "    #convert to grayscale\n",
    "    #gray = img.convert('L')\n",
    "    #gray.save(NEWDATA_DIR+'\\\\'+file,\"JPEG\")\n",
    "print(bad_indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from sklearn.externals import joblib \n",
    "def CNN_training(x_train, y_train):\n",
    "    \n",
    "        \n",
    "    x_train = np.array(x_train)\n",
    "    x_train = x_train/255.0 #normalize\n",
    "    \n",
    "    \n",
    "    y_train = np.array(y_train)\n",
    "    \n",
    "    \n",
    "    #start\n",
    "    model = Sequential()\n",
    "    #insert a conv layer that accepts the image data x2\n",
    "    \n",
    "    ### READ THIS ####\n",
    "    #You had an error in the input shape, where you se (50, 50 ,1) and it should be (50, 50, 3)\n",
    "    # remember you need to be consisted with the size\n",
    "    model.add(keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(100, 100,3)))\n",
    "    \n",
    "    #max pool layer for further processing x2\n",
    "    model.add(keras.layers.MaxPooling2D((2,2)))\n",
    "    model.add(keras.layers.Conv2D(128,(3,3),activation='relu'))\n",
    "    model.add(keras.layers.MaxPooling2D((2,2)))\n",
    "    \n",
    "    #flatten data for full-connected layer\n",
    "    model.add(keras.layers.Flatten())\n",
    "    \n",
    "    #regularization\n",
    "    model.add(keras.layers.Dropout(0.5))\n",
    "    model.add(keras.layers.Dense(512,activation='relu'))\n",
    "    model.add(keras.layers.Dense(2,activation='sigmoid'))\n",
    "    model.compile(optimizer='rmsprop',\n",
    "                 loss='binary_crossentropy',\n",
    "                 metrics=['accuracy'])\n",
    "    #ntrain = len(x_train)\n",
    "    #nval = len(x_test)\n",
    "    \n",
    "    \n",
    "    model.fit(x_train,y_train,epochs=1,batch_size=32)\n",
    "\n",
    "    model.save('CNN_testerSwamp.sav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(84, 100, 100, 3)\n",
      "(84,)\n",
      "[[0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]]\n",
      "<HDF5 dataset \"train_img\": shape (84, 100, 100, 3), type \"<f4\">\n",
      "84/84 [==============================] - 2s 23ms/sample - loss: 0.2686 - acc: 0.7321\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test = r\"./dsSwampers.hdf5\"\n",
    "# read the hdf5 or h5 time, use mode \"r\" for \"Only Read\"\n",
    "f = h5py.File(test, mode=\"r\")\n",
    "x, y = f.values()\n",
    "print(x.shape)\n",
    "print(y.shape)\n",
    "y_binary = to_categorical(y)\n",
    "print(y_binary)\n",
    "print(x)\n",
    "CNN_training(x,y_binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PIL.Image.Image image mode=RGB size=350x350 at 0x2293BE342B0>\n",
      "[[[ 57.  57.  57.]\n",
      "  [ 57.  57.  57.]\n",
      "  [ 58.  58.  58.]\n",
      "  ...\n",
      "  [ 43.  43.  43.]\n",
      "  [ 48.  48.  48.]\n",
      "  [ 43.  43.  43.]]\n",
      "\n",
      " [[ 57.  57.  57.]\n",
      "  [ 56.  56.  56.]\n",
      "  [ 58.  58.  58.]\n",
      "  ...\n",
      "  [ 42.  42.  42.]\n",
      "  [ 48.  48.  48.]\n",
      "  [ 47.  47.  47.]]\n",
      "\n",
      " [[ 57.  57.  57.]\n",
      "  [ 56.  56.  56.]\n",
      "  [ 57.  57.  57.]\n",
      "  ...\n",
      "  [ 43.  43.  43.]\n",
      "  [ 45.  45.  45.]\n",
      "  [ 47.  47.  47.]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 59.  59.  59.]\n",
      "  [ 59.  59.  59.]\n",
      "  [ 61.  61.  61.]\n",
      "  ...\n",
      "  [ 93.  93.  93.]\n",
      "  [ 97.  97.  97.]\n",
      "  [ 92.  92.  92.]]\n",
      "\n",
      " [[ 53.  53.  53.]\n",
      "  [ 52.  52.  52.]\n",
      "  [ 53.  53.  53.]\n",
      "  ...\n",
      "  [ 97.  97.  97.]\n",
      "  [ 99.  99.  99.]\n",
      "  [ 94.  94.  94.]]\n",
      "\n",
      " [[ 48.  48.  48.]\n",
      "  [ 48.  48.  48.]\n",
      "  [ 46.  46.  46.]\n",
      "  ...\n",
      "  [ 99.  99.  99.]\n",
      "  [102. 102. 102.]\n",
      "  [ 95.  95.  95.]]]\n",
      "<PIL.Image.Image image mode=RGB size=350x350 at 0x228DE555668>\n",
      "[[[ 44.5625  44.5625  44.5625]\n",
      "  [  4.1875   4.1875   4.1875]\n",
      "  [  6.25     6.25     6.25  ]\n",
      "  ...\n",
      "  [ 14.25    14.25    14.25  ]\n",
      "  [ 14.25    14.25    14.25  ]\n",
      "  [ 15.8125  15.8125  15.8125]]\n",
      "\n",
      " [[ 44.      44.      44.    ]\n",
      "  [  5.9375   5.9375   5.9375]\n",
      "  [  7.1875   7.1875   7.1875]\n",
      "  ...\n",
      "  [ 19.      19.      19.    ]\n",
      "  [ 18.5     18.5     18.5   ]\n",
      "  [ 17.      17.      17.    ]]\n",
      "\n",
      " [[ 42.5     42.5     42.5   ]\n",
      "  [  7.25     7.25     7.25  ]\n",
      "  [  8.       8.       8.    ]\n",
      "  ...\n",
      "  [ 19.      19.      19.    ]\n",
      "  [ 19.75    19.75    19.75  ]\n",
      "  [ 15.25    15.25    15.25  ]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[135.8125 135.8125 135.8125]\n",
      "  [133.25   133.25   133.25  ]\n",
      "  [136.6875 136.6875 136.6875]\n",
      "  ...\n",
      "  [108.5625 108.5625 108.5625]\n",
      "  [115.25   115.25   115.25  ]\n",
      "  [108.6875 108.6875 108.6875]]\n",
      "\n",
      " [[136.75   136.75   136.75  ]\n",
      "  [133.0625 133.0625 133.0625]\n",
      "  [134.0625 134.0625 134.0625]\n",
      "  ...\n",
      "  [ 78.25    78.25    78.25  ]\n",
      "  [ 95.8125  95.8125  95.8125]\n",
      "  [109.375  109.375  109.375 ]]\n",
      "\n",
      " [[138.5625 138.5625 138.5625]\n",
      "  [136.75   136.75   136.75  ]\n",
      "  [136.     136.     136.    ]\n",
      "  ...\n",
      "  [ 44.25    44.25    44.25  ]\n",
      "  [ 57.25    57.25    57.25  ]\n",
      "  [ 77.5     77.5     77.5   ]]]\n",
      "[1]\n",
      "[1]\n"
     ]
    }
   ],
   "source": [
    "img2 = keras.preprocessing.image.load_img('C:\\\\Users\\\\JohnQ\\\\Documents\\\\Hackathons\\\\SwampHacks2020\\\\images\\\\Sourav_Ganguly_0004.jpg') # this is a PIL image\n",
    "print(img2)\n",
    "array2 = keras.preprocessing.image.img_to_array(img)    \n",
    "arrayresized2 = cv2.resize(array2, (100,100))\n",
    "print(arrayresized2) # this is a Numpy array with shape (3, 150, 150)\n",
    "inputarray2 = arrayresized[np.newaxis,...] # dimension added to fit input size\n",
    "img2 = keras.preprocessing.image.load_img('C:\\\\Users\\\\JohnQ\\\\Documents\\\\Hackathons\\\\SwampHacks2020\\\\images\\\\Spencer_Abraham_0005.jpg') # this is a PIL image\n",
    "print(img2)\n",
    "array2 = keras.preprocessing.image.img_to_array(img2)    \n",
    "arrayresized2 = cv2.resize(array, (100,100))\n",
    "print(arrayresized2) # this is a Numpy array with shape (3, 150, 150)\n",
    "inputarray = arrayresized[np.newaxis,...] # dimension added to fit input size\n",
    "net = tf.keras.models.load_model(r'C:\\Users\\JohnQ\\Documents\\Hackathons\\SwampHacks2020\\CNN_testerSwamp.sav')\n",
    "\n",
    "print(net.predict_classes(inputarray2))\n",
    "print(net.predict_classes(inputarray))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can't pickle _thread.RLock objects",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-114-23c95ce4dab4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mcnn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'CNN_tester.model'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#cnn = \"C:\\\\Users\\\\JohnQ\\\\Documents\\\\Hackathons\\\\SwampHacks2020\\\\CNN_tester2.sav\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0msaved_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcnn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mcnn_from_pickle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msaved_model\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcnn_from_pickle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: can't pickle _thread.RLock objects"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from sklearn.externals import joblib\n",
    "from keras.models import load_model\n",
    "cnn = keras.models.load_model('CNN_tester.model')\n",
    "#cnn = \"C:\\\\Users\\\\JohnQ\\\\Documents\\\\Hackathons\\\\SwampHacks2020\\\\CNN_tester2.sav\"\n",
    "saved_model = pickle.dumps(cnn)\n",
    "cnn_from_pickle = pickle.loads(saved_model) \n",
    "print(cnn_from_pickle)  \n",
    "# Save the model as a pickle in a file \n",
    "joblib.dump(cnn, 'modelSwampers.pkl') \n",
    "  \n",
    "# Load the model from the file \n",
    "cnn_from_joblib = joblib.load('modelSwampers.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
